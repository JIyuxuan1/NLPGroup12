{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: weave in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (0.51.46)\n",
      "Requirement already satisfied: wandb in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (0.19.11)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (5.6.3)\n",
      "Requirement already satisfied: emoji>=2.12.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (2.14.1)\n",
      "Requirement already satisfied: gql[aiohttp,requests] in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (3.5.2)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (4.23.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (1.6.0)\n",
      "Requirement already satisfied: numpy>1.21.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (1.26.3)\n",
      "Requirement already satisfied: packaging>=21.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (24.1)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (2.11.4)\n",
      "Requirement already satisfied: rich in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (14.0.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils>=0.9.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from weave) (0.10.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (8.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (6.30.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (2.28.0)\n",
      "Requirement already satisfied: setproctitle in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (72.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from jsonschema>=4.23.0->weave) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: graphql-core<3.2.5,>=3.2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (3.2.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (1.20.0)\n",
      "Requirement already satisfied: backoff<3.0,>=1.11.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (2.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (4.9.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.9.0b0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (3.11.18)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gql[aiohttp,requests]->weave) (1.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from rich->weave) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from rich->weave) (2.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave) (0.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->weave) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: transformers in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (4.50.0.dev0)\n",
      "Requirement already satisfied: pandas in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: datasets in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: six>=1.5 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install weave wandb\n",
    "%pip install torch transformers pandas scikit-learn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "import time\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "config = {\n",
    "    \"model_name\": 'roberta-base',\n",
    "    \"max_len\": 256,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 3,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"test_set_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"num_labels\": 2,\n",
    "    \"wandb_project\": \"fake-news-roberta-classification-v2\",\n",
    "    \"wandb_entity\": None,\n",
    "    \"csv_filename\": \"train.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"7bb63202e57cfbdea2fb28ef9f2c7b78b422c69d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/yaohli/workspace/matheval_projects/datasets/nlpclass/NLPGroup12/wandb/run-20250513_144418-szu60jwn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiyuxuan0918-university-of-technology-sydney/roberta-base/runs/szu60jwn' target=\"_blank\">deep-resonance-6</a></strong> to <a href='https://wandb.ai/jiyuxuan0918-university-of-technology-sydney/roberta-base' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiyuxuan0918-university-of-technology-sydney/roberta-base' target=\"_blank\">https://wandb.ai/jiyuxuan0918-university-of-technology-sydney/roberta-base</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiyuxuan0918-university-of-technology-sydney/roberta-base/runs/szu60jwn' target=\"_blank\">https://wandb.ai/jiyuxuan0918-university-of-technology-sydney/roberta-base/runs/szu60jwn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=config[\"model_name\"],\n",
    "    entity=config[\"wandb_entity\"],\n",
    "    config=config\n",
    ")\n",
    "print(\"W&B Initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/yaohli/workspace/matheval_projects/datasets/nlpclass/NLPGroup12'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "raw_dataset = load_dataset('csv', data_files=config['csv_filename'], split='train')\n",
    "\n",
    "def combine_text_features(examples):\n",
    "    titles = [str(t) if t is not None else \"\" for t in examples['title']]\n",
    "    texts = [str(t) if t is not None else \"\" for t in examples['text']]\n",
    "    examples['full_text'] = [title + \" [SEP] \" + text for title, text in zip(titles, texts)]\n",
    "    return examples\n",
    "\n",
    "raw_dataset = raw_dataset.map(combine_text_features, batched=True, remove_columns=['title', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing data...\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(config['model_name'])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['full_text'], truncation=True, padding=False, max_length=config['max_len'])\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=['full_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 57106/57106 [00:00<00:00, 88244.66 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if 'label' in tokenized_dataset.column_names and 'labels' not in tokenized_dataset.column_names:\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.cast_column(\"labels\", ClassLabel(num_classes=config['num_labels'], names=['Fake (0)', 'Real (1)']))\n",
    "\n",
    "print(\"Splitting dataset...\")\n",
    "split_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=config[\"test_set_size\"],\n",
    "    seed=config[\"random_state\"],\n",
    "    stratify_by_column=\"labels\"\n",
    ")\n",
    "\n",
    "split_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45684 train / 11422 test samples\n"
     ]
    }
   ],
   "source": [
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "label_names = ['Fake (0)', 'Real (1)']\n",
    "\n",
    "print(f\"{len(train_dataset)} train / {len(test_dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    RobertaForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaohli/Data/conda_space/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    config['model_name'],\n",
    "    num_labels=config['num_labels']\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * config['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoints will be saved to: ./roberta-base_checkpoints\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "  Train Batch 100/2856\n",
      "  Train Batch 200/2856\n",
      "  Train Batch 300/2856\n",
      "  Train Batch 400/2856\n",
      "  Train Batch 500/2856\n",
      "  Train Batch 600/2856\n",
      "  Train Batch 700/2856\n",
      "  Train Batch 800/2856\n",
      "  Train Batch 900/2856\n",
      "  Train Batch 1000/2856\n",
      "  Train Batch 1100/2856\n",
      "  Train Batch 1200/2856\n",
      "  Train Batch 1300/2856\n",
      "  Train Batch 1400/2856\n",
      "  Train Batch 1500/2856\n",
      "  Train Batch 1600/2856\n",
      "  Train Batch 1700/2856\n",
      "  Train Batch 1800/2856\n",
      "  Train Batch 1900/2856\n",
      "  Train Batch 2000/2856\n",
      "  Train Batch 2100/2856\n",
      "  Train Batch 2200/2856\n",
      "  Train Batch 2300/2856\n",
      "  Train Batch 2400/2856\n",
      "  Train Batch 2500/2856\n",
      "  Train Batch 2600/2856\n",
      "  Train Batch 2700/2856\n",
      "  Train Batch 2800/2856\n",
      "--- Evaluating Epoch 1 ---\n",
      "Epoch 1 | Train Loss: 0.1617 | Val Loss: 0.1398 | Val Acc: 0.9610 | Time: 992.83s\n",
      "  New best val accuracy: 0.9610. Saving model to ./roberta-base_checkpoints\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "  Train Batch 100/2856\n",
      "  Train Batch 200/2856\n",
      "  Train Batch 300/2856\n",
      "  Train Batch 400/2856\n",
      "  Train Batch 500/2856\n",
      "  Train Batch 600/2856\n",
      "  Train Batch 700/2856\n",
      "  Train Batch 800/2856\n",
      "  Train Batch 900/2856\n",
      "  Train Batch 1000/2856\n",
      "  Train Batch 1100/2856\n",
      "  Train Batch 1200/2856\n",
      "  Train Batch 1300/2856\n",
      "  Train Batch 1400/2856\n",
      "  Train Batch 1500/2856\n",
      "  Train Batch 1600/2856\n",
      "  Train Batch 1700/2856\n",
      "  Train Batch 1800/2856\n",
      "  Train Batch 1900/2856\n",
      "  Train Batch 2000/2856\n",
      "  Train Batch 2100/2856\n",
      "  Train Batch 2200/2856\n",
      "  Train Batch 2300/2856\n",
      "  Train Batch 2400/2856\n",
      "  Train Batch 2500/2856\n",
      "  Train Batch 2600/2856\n",
      "  Train Batch 2700/2856\n",
      "  Train Batch 2800/2856\n",
      "--- Evaluating Epoch 2 ---\n",
      "Epoch 2 | Train Loss: 0.1409 | Val Loss: 0.1298 | Val Acc: 0.9628 | Time: 956.15s\n",
      "  New best val accuracy: 0.9628. Saving model to ./roberta-base_checkpoints\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "  Train Batch 100/2856\n",
      "  Train Batch 200/2856\n",
      "  Train Batch 300/2856\n",
      "  Train Batch 400/2856\n",
      "  Train Batch 500/2856\n",
      "  Train Batch 600/2856\n",
      "  Train Batch 700/2856\n",
      "  Train Batch 800/2856\n",
      "  Train Batch 900/2856\n",
      "  Train Batch 1000/2856\n",
      "  Train Batch 1100/2856\n",
      "  Train Batch 1200/2856\n",
      "  Train Batch 1300/2856\n",
      "  Train Batch 1400/2856\n",
      "  Train Batch 1500/2856\n",
      "  Train Batch 1600/2856\n",
      "  Train Batch 1700/2856\n",
      "  Train Batch 1800/2856\n",
      "  Train Batch 1900/2856\n",
      "  Train Batch 2000/2856\n",
      "  Train Batch 2100/2856\n",
      "  Train Batch 2200/2856\n",
      "  Train Batch 2300/2856\n",
      "  Train Batch 2400/2856\n",
      "  Train Batch 2500/2856\n",
      "  Train Batch 2600/2856\n",
      "  Train Batch 2700/2856\n",
      "  Train Batch 2800/2856\n",
      "--- Evaluating Epoch 3 ---\n",
      "Epoch 3 | Train Loss: 0.1360 | Val Loss: 0.1305 | Val Acc: 0.9631 | Time: 977.65s\n",
      "  New best val accuracy: 0.9631. Saving model to ./roberta-base_checkpoints\n",
      "\n",
      "Total Training Time: 2936.98s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2856. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 5712. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 8568. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "import time\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os # Import os for directory handling\n",
    "\n",
    "# Assume 'config', 'model', 'train_loader', 'test_loader', 'optimizer', 'scheduler', 'device', 'label_names', 'tokenizer' are defined\n",
    "\n",
    "# --- Model Saving Setup ---\n",
    "model_save_path = f\"./{config['model_name']}_checkpoints\"\n",
    "os.makedirs(model_save_path, exist_ok=True) # Create directory if it doesn't exist\n",
    "print(f\"Model checkpoints will be saved to: {model_save_path}\")\n",
    "# --- End Model Saving Setup ---\n",
    "\n",
    "\n",
    "print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "overall_start_time = time.time()\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "# Variables to store results from the last epoch for final reporting\n",
    "final_predictions = []\n",
    "final_true_labels = []\n",
    "final_avg_eval_loss = 0.0\n",
    "final_accuracy = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{config['epochs']} ---\")\n",
    "\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch_num, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if loss is None or torch.isnan(loss) or torch.isinf(loss):\n",
    "            # Basic check for invalid loss\n",
    "            print(f\"Warning: Invalid loss in train batch {batch_num}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if wandb.run:\n",
    "             wandb.log({\"train_loss_step\": loss.item(), \"learning_rate_step\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "        # Print less frequently\n",
    "        if (batch_num + 1) % 100 == 0:\n",
    "             print(f\"  Train Batch {batch_num + 1}/{len(train_loader)}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(f\"--- Evaluating Epoch {epoch + 1} ---\")\n",
    "    model.eval()\n",
    "    epoch_predictions = []\n",
    "    epoch_true_labels = []\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            if loss is not None:\n",
    "                total_eval_loss += loss.item()\n",
    "\n",
    "            batch_preds = torch.argmax(logits, dim=1)\n",
    "            epoch_predictions.extend(batch_preds.cpu().tolist())\n",
    "            epoch_true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(test_loader) if len(test_loader) > 0 else 0.0\n",
    "\n",
    "    # --- Calculate Metrics ---\n",
    "    if not epoch_true_labels or not epoch_predictions:\n",
    "        print(\"Warning: No evaluation predictions made this epoch.\")\n",
    "        accuracy, precision_weighted, recall_weighted, f1_weighted = 0.0, 0.0, 0.0, 0.0\n",
    "        precision_macro, recall_macro, f1_macro = 0.0, 0.0, 0.0\n",
    "    else:\n",
    "        accuracy = accuracy_score(epoch_true_labels, epoch_predictions)\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "            epoch_true_labels, epoch_predictions, average='weighted', zero_division=0\n",
    "        )\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            epoch_true_labels, epoch_predictions, average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_eval_loss:.4f} | Val Acc: {accuracy:.4f} | Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # --- Log epoch metrics to W&B ---\n",
    "    if wandb.run:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"avg_train_loss\": avg_train_loss,\n",
    "            \"avg_val_loss\": avg_eval_loss,\n",
    "            \"val_accuracy\": accuracy,\n",
    "            \"val_precision_weighted\": precision_weighted,\n",
    "            \"val_recall_weighted\": recall_weighted,\n",
    "            \"val_f1_weighted\": f1_weighted,\n",
    "            \"val_precision_macro\": precision_macro,\n",
    "            \"val_recall_macro\": recall_macro,\n",
    "            \"val_f1_macro\": f1_macro,\n",
    "            \"epoch_duration_sec\": epoch_time,\n",
    "        }, step=epoch + 1)\n",
    "\n",
    "    # --- Check for improvement and Save Model ---\n",
    "    if accuracy > best_val_accuracy:\n",
    "        print(f\"  New best val accuracy: {accuracy:.4f}. Saving model to {model_save_path}\")\n",
    "        best_val_accuracy = accuracy\n",
    "        # Save the model and tokenizer\n",
    "        model.save_pretrained(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        # --- End Model Saving ---\n",
    "\n",
    "    # Store results from the last epoch for final report outside the loop\n",
    "    if epoch == config['epochs'] - 1:\n",
    "        final_predictions = epoch_predictions\n",
    "        final_true_labels = epoch_true_labels\n",
    "        final_avg_eval_loss = avg_eval_loss\n",
    "        final_accuracy = accuracy\n",
    "\n",
    "\n",
    "# --- End of Training Loop ---\n",
    "total_training_time = time.time() - overall_start_time\n",
    "print(f\"\\nTotal Training Time: {total_training_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Fake (0)     0.9987    0.9308    0.9636      6000\n",
      "    Real (1)     0.9288    0.9987    0.9625      5422\n",
      "\n",
      "    accuracy                         0.9631     11422\n",
      "   macro avg     0.9638    0.9648    0.9630     11422\n",
      "weighted avg     0.9656    0.9631    0.9631     11422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "if not final_true_labels or not final_predictions:\n",
    "     final_report_string = \"N/A - No predictions available.\"\n",
    "else:\n",
    "    final_report_string = classification_report(\n",
    "        final_true_labels,\n",
    "        final_predictions,\n",
    "        target_names=label_names,\n",
    "        zero_division=0,\n",
    "        digits=4\n",
    "    )\n",
    "print(final_report_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
